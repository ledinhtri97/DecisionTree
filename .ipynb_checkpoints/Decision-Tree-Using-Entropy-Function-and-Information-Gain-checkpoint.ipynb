{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID3 (Iterative Dichotomiser 3) \n",
    "#â†’ uses Entropy function and Information gain as metrics.\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.compute the entropy for data-set <br>\n",
    "\n",
    "2.for every attribute/feature: <br>\n",
    "   \n",
    "->1.calculate entropy for all categorical values <br>\n",
    "->2.take average information entropy for the current attribute <br>\n",
    "->3.calculate gain for the current attribute <br>\n",
    "\n",
    "3.pick the highest gain attribute.<br>\n",
    "\n",
    "4.Repeat until we get the tree we desired.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Dataset Zoo Data Set:\n",
    "#link: http://archive.ics.uci.edu/ml/datasets/zoo\n",
    "\n",
    "Number of Instances: 101\n",
    "Number of Attributes: 18 (animal name, 15 Boolean attributes, 2 numerics)\n",
    "\n",
    "41 dong vat co vu \n",
    "\n",
    "20 dong vat gia cam\n",
    "\n",
    "5  dong vat bo sat\n",
    "\n",
    "13 dong vat thuy san\n",
    "\n",
    "4  dong vat luong cu\n",
    "\n",
    "8  dong vat con trung\n",
    "\n",
    "10 dong vat than mem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'legs': {'0': {'fins': {'0': {'toothed': {'0': '7', '1': '3'}},\n",
      "                         '1': {'eggs': {'0': '1', '1': '4'}}}},\n",
      "          '2': {'hair': {'0': '2', '1': '1'}},\n",
      "          '4': {'hair': {'0': {'toothed': {'0': '7', '1': '5'}}, '1': '1'}},\n",
      "          '6': {'aquatic': {'0': '6', '1': '7'}},\n",
      "          '8': '7',\n",
      "          'legs': 'type'}}\n",
      "The prediction accuracy is:  86.36363636363636 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "#Import the dataset and define the feature as well as the target datasets / columns#\n",
    "dataset = pd.read_csv('zoo.csv',\n",
    "                      names=['animal_name','hair','feathers','eggs','milk',\n",
    "                                                   'airbone','aquatic','predator','toothed','backbone',\n",
    "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])\n",
    "\n",
    "dataset=dataset.drop('animal_name',axis=1)\n",
    "############################################\n",
    "def entropy(target_col):\n",
    "    #tra ve cac phan tu rieng biet va so luong cua moi phan tu\n",
    "    elements,counts = np.unique(target_col,return_counts = True)\n",
    "    \n",
    "    #tinh entropy cua target_col, E=xichma[-plog2(p)]\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy \n",
    "    \n",
    "#############################################\n",
    "def InfoGain(data,split_attribute_name,target_name=\"class\"):\n",
    " \n",
    "    #Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    ##Calculate the entropy of the dataset\n",
    "    \n",
    "    #Calculate the values and the corresponding counts for the split attribute \n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
    "    \n",
    "    #Calculate the weighted entropy\n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    \n",
    "    #Calculate the information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain\n",
    "       \n",
    "\n",
    "#############################################\n",
    "\n",
    "def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None): \n",
    "    #Define the stopping criteria --> If one of this is satisfied, return a leaf node#\n",
    "    \n",
    "    #If all target_values have the same value, return this value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
    "    elif len(data)==0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
    "\n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    #If none of the above holds true, grow the tree!\n",
    "    \n",
    "    else:\n",
    "        #Set the default value for this node --> The mode target feature value of the current node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        #Select the feature which best splits the dataset\n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
    "        #gain in the first run\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        \n",
    "        #Remove the feature with the best inforamtion gain from the feature space\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        #Grow a branch under the root node for each possible value of the root node feature\n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
    "            \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)    \n",
    "                 \n",
    "    \n",
    "def predict(query,tree,default = 1):\n",
    "    \n",
    "    #1.\n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            #2.\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "  \n",
    "            #3.\n",
    "            result = tree[key][query[key]]\n",
    "            #4.\n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "            else:\n",
    "                return result\n",
    "        \n",
    "        \n",
    "def train_test_split(dataset):\n",
    "    training_data = dataset.iloc[:80].reset_index(drop=True)#drop the index respectively relabel the index\n",
    "    #starting form 0, in other to do not want to run into errors regarding the row labels / indexes\n",
    "    testing_data = dataset.iloc[80:].reset_index(drop=True)\n",
    "    return training_data,testing_data\n",
    "training_data = train_test_split(dataset)[0]\n",
    "testing_data = train_test_split(dataset)[1] \n",
    "def test(data,tree):\n",
    "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
    "    #convert it to a dictionary\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    \n",
    "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    \n",
    "    #Calculate the prediction accuracy\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
    "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')\n",
    "    \n",
    "\"\"\"\n",
    "Train the tree, Print the tree and predict the accuracy\n",
    "\"\"\"\n",
    "tree = ID3(training_data,training_data,training_data.columns[:-1])\n",
    "pprint(tree)\n",
    "test(testing_data,tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is:  80.95238095238095 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Import the DecisionTreeClassifier model.\n",
    "\"\"\"\n",
    "#Import the DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\"\"\"\n",
    "Import the Zoo Dataset\n",
    "\"\"\"\n",
    "#Import the dataset \n",
    "dataset = pd.read_csv('zoo.csv')\n",
    "#We drop the animal names since this is not a good feature to split the data on\n",
    "dataset=dataset.drop('animal_name',axis=1)\n",
    "\n",
    "\"\"\"\n",
    "Split the data into a training and a testing set\n",
    "\"\"\"\n",
    "train_features = dataset.iloc[:80,:-1]\n",
    "test_features = dataset.iloc[80:,:-1]\n",
    "train_targets = dataset.iloc[:80,-1]\n",
    "test_targets = dataset.iloc[80:,-1]\n",
    "\n",
    "\"\"\"\n",
    "Train the model\n",
    "\"\"\"\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy').fit(train_features,train_targets)\n",
    "\n",
    "\"\"\"\n",
    "Predict the classes of new, unseen data\n",
    "\"\"\"\n",
    "prediction = tree.predict(test_features)\n",
    "\n",
    "###########################################################################################################\n",
    "\"\"\"\n",
    "Check the accuracy\n",
    "\"\"\"\n",
    "print(\"The prediction accuracy is: \",tree.score(test_features,test_targets)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
